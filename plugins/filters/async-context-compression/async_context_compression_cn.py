"""
title: å¼‚æ­¥ä¸Šä¸‹æ–‡å‹ç¼©
id: async_context_compression
author: Fu-Jie
author_url: https://github.com/Fu-Jie/awesome-openwebui
funding_url: https://github.com/open-webui
description: é€šè¿‡æ™ºèƒ½æ‘˜è¦å’Œæ¶ˆæ¯å‹ç¼©ï¼Œé™ä½é•¿å¯¹è¯çš„ token æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒå¯¹è¯è¿è´¯æ€§ã€‚
version: 1.2.0
openwebui_id: 5c0617cb-a9e4-4bd6-a440-d276534ebd18
license: MIT

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Œ 1.2.0 ç‰ˆæœ¬æ›´æ–°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  âœ… é¢„æ£€ä¸Šä¸‹æ–‡æ£€æŸ¥ï¼šå‘é€ç»™æ¨¡å‹å‰éªŒè¯ä¸Šä¸‹æ–‡æ˜¯å¦é€‚é…ã€‚
  âœ… ç»“æ„æ„ŸçŸ¥è£å‰ªï¼šæŠ˜å è¿‡é•¿çš„ AI å“åº”ï¼ŒåŒæ—¶ä¿ç•™æ ‡é¢˜ (H1-H6)ã€å¼€å¤´å’Œç»“å°¾ã€‚
  âœ… åŸç”Ÿå·¥å…·è¾“å‡ºè£å‰ªï¼šä½¿ç”¨å‡½æ•°è°ƒç”¨æ—¶æ¸…ç†ä¸Šä¸‹æ–‡ï¼Œå»é™¤å†—ä½™è¾“å‡ºã€‚ï¼ˆæ³¨æ„ï¼šéåŸç”Ÿå·¥å…·è°ƒç”¨è¾“å‡ºä¸ä¼šå®Œæ•´æ³¨å…¥ä¸Šä¸‹æ–‡ï¼‰
  âœ… ä¸Šä¸‹æ–‡ä½¿ç”¨è­¦å‘Šï¼šå½“ä½¿ç”¨é‡è¶…è¿‡ 90% æ—¶å‘å‡ºé€šçŸ¥ã€‚
  âœ… è¯¦ç»† Token æ—¥å¿—ï¼šç»†ç²’åº¦è®°å½• Systemã€Headã€Summary å’Œ Tail çš„ Token æ¶ˆè€—ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Œ åŠŸèƒ½æ¦‚è¿°
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æœ¬è¿‡æ»¤å™¨é€šè¿‡æ™ºèƒ½æ‘˜è¦å’Œæ¶ˆæ¯å‹ç¼©æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½é•¿å¯¹è¯çš„ token æ¶ˆè€—ï¼ŒåŒæ—¶ä¿æŒå¯¹è¯è¿è´¯æ€§ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
  âœ… è‡ªåŠ¨è§¦å‘å‹ç¼©ï¼ˆåŸºäº Token æ•°é‡é˜ˆå€¼ï¼‰
  âœ… å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼ˆä¸é˜»å¡ç”¨æˆ·å“åº”ï¼‰
  âœ… æ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨ï¼ˆæ”¯æŒ PostgreSQL å’Œ SQLiteï¼‰
  âœ… çµæ´»çš„ä¿ç•™ç­–ç•¥ï¼ˆå¯é…ç½®ä¿ç•™å¯¹è¯çš„å¤´éƒ¨å’Œå°¾éƒ¨ï¼‰
  âœ… æ™ºèƒ½æ³¨å…¥æ‘˜è¦ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ”„ å·¥ä½œæµç¨‹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é˜¶æ®µ 1: inletï¼ˆè¯·æ±‚å‰å¤„ç†ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  1. æ¥æ”¶å½“å‰å¯¹è¯çš„æ‰€æœ‰æ¶ˆæ¯ã€‚
  2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„æ‘˜è¦ã€‚
  3. å¦‚æœæœ‰æ‘˜è¦ä¸”æ¶ˆæ¯æ•°è¶…è¿‡ä¿ç•™é˜ˆå€¼ï¼š
     â”œâ”€ æå–è¦ä¿ç•™çš„å¤´éƒ¨æ¶ˆæ¯ï¼ˆä¾‹å¦‚ï¼Œç¬¬ä¸€æ¡æ¶ˆæ¯ï¼‰ã€‚
     â”œâ”€ å°†æ‘˜è¦æ³¨å…¥åˆ°å¤´éƒ¨æ¶ˆæ¯ä¸­ã€‚
     â”œâ”€ æå–è¦ä¿ç•™çš„å°¾éƒ¨æ¶ˆæ¯ã€‚
     â””â”€ ç»„åˆæˆæ–°çš„æ¶ˆæ¯åˆ—è¡¨ï¼š[å¤´éƒ¨æ¶ˆæ¯+æ‘˜è¦] + [å°¾éƒ¨æ¶ˆæ¯]ã€‚
  4. å‘é€å‹ç¼©åçš„æ¶ˆæ¯åˆ° LLMã€‚

é˜¶æ®µ 2: outletï¼ˆå“åº”åå¤„ç†ï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  1. LLM å“åº”å®Œæˆåè§¦å‘ã€‚
  2. æ£€æŸ¥ Token æ•°æ˜¯å¦è¾¾åˆ°å‹ç¼©é˜ˆå€¼ã€‚
  3. å¦‚æœè¾¾åˆ° Token é˜ˆå€¼ï¼Œåˆ™åœ¨åå°å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼š
     â”œâ”€ æå–éœ€è¦æ‘˜è¦çš„æ¶ˆæ¯ï¼ˆæ’é™¤ä¿ç•™çš„å¤´éƒ¨å’Œå°¾éƒ¨ï¼‰ã€‚
     â”œâ”€ è°ƒç”¨ LLM ç”Ÿæˆç®€æ´æ‘˜è¦ã€‚
     â””â”€ å°†æ‘˜è¦ä¿å­˜åˆ°æ•°æ®åº“ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¾ å­˜å‚¨æ–¹æ¡ˆ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æœ¬è¿‡æ»¤å™¨ä½¿ç”¨ Open WebUI çš„å…±äº«æ•°æ®åº“è¿æ¥è¿›è¡ŒæŒä¹…åŒ–å­˜å‚¨ã€‚
å®ƒè‡ªåŠ¨å¤ç”¨ Open WebUI å†…éƒ¨çš„ SQLAlchemy å¼•æ“å’Œ SessionLocalï¼Œ
ä½¿æ’ä»¶ä¸æ•°æ®åº“ç±»å‹æ— å…³ï¼Œå¹¶ç¡®ä¿ä¸ Open WebUI æ”¯æŒçš„ä»»ä½•æ•°æ®åº“åç«¯
ï¼ˆPostgreSQLã€SQLite ç­‰ï¼‰å…¼å®¹ã€‚

æ— éœ€é¢å¤–çš„æ•°æ®åº“é…ç½® - æ’ä»¶è‡ªåŠ¨ç»§æ‰¿ Open WebUI çš„æ•°æ®åº“è®¾ç½®ã€‚

  è¡¨ç»“æ„ï¼š
    - id: ä¸»é”®ï¼ˆè‡ªå¢ï¼‰
    - chat_id: å¯¹è¯å”¯ä¸€æ ‡è¯†ï¼ˆå”¯ä¸€ç´¢å¼•ï¼‰
    - summary: æ‘˜è¦å†…å®¹ï¼ˆTEXTï¼‰
    - compressed_message_count: åŸå§‹æ¶ˆæ¯æ•°
    - created_at: åˆ›å»ºæ—¶é—´
    - updated_at: æ›´æ–°æ—¶é—´

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š å‹ç¼©æ•ˆæœç¤ºä¾‹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åœºæ™¯ï¼š20 æ¡æ¶ˆæ¯çš„å¯¹è¯ (é»˜è®¤è®¾ç½®: ä¿ç•™å‰ 1 æ¡, å 6 æ¡)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  å‹ç¼©å‰ï¼š
    æ¶ˆæ¯ 1: [åˆå§‹è®¾å®š + åˆå§‹é—®é¢˜]
    æ¶ˆæ¯ 2-14: [å†å²å¯¹è¯å†…å®¹]
    æ¶ˆæ¯ 15-20: [æœ€è¿‘å¯¹è¯]
    æ€»è®¡: 20 æ¡å®Œæ•´æ¶ˆæ¯

  å‹ç¼©åï¼š
    æ¶ˆæ¯ 1: [åˆå§‹è®¾å®š + å†å²æ‘˜è¦ + åˆå§‹é—®é¢˜]
    æ¶ˆæ¯ 15-20: [æœ€è¿‘ 6 æ¡å®Œæ•´æ¶ˆæ¯]
    æ€»è®¡: 7 æ¡æ¶ˆæ¯

  æ•ˆæœï¼š
    âœ“ èŠ‚çœ 13 æ¡æ¶ˆæ¯ï¼ˆçº¦ 65%ï¼‰
    âœ“ ä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯
    âœ“ ä¿æŠ¤é‡è¦çš„åˆå§‹è®¾å®š

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš™ï¸ é…ç½®å‚æ•°è¯´æ˜
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

priority (ä¼˜å…ˆçº§)
  é»˜è®¤: 10
  è¯´æ˜: è¿‡æ»¤å™¨æ‰§è¡Œé¡ºåºï¼Œæ•°å€¼è¶Šå°è¶Šå…ˆæ‰§è¡Œã€‚

compression_threshold_tokens (å‹ç¼©é˜ˆå€¼ Token)
  é»˜è®¤: 64000
  è¯´æ˜: å½“ä¸Šä¸‹æ–‡æ€» Token æ•°è¶…è¿‡æ­¤å€¼æ—¶ï¼Œè§¦å‘å‹ç¼©ã€‚
  å»ºè®®: æ ¹æ®æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å’Œæˆæœ¬è°ƒæ•´ã€‚

max_context_tokens (æœ€å¤§ä¸Šä¸‹æ–‡ Token)
  é»˜è®¤: 128000
  è¯´æ˜: ä¸Šä¸‹æ–‡çš„ç¡¬æ€§ä¸Šé™ã€‚è¶…è¿‡æ­¤å€¼å°†å¼ºåˆ¶ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯ã€‚

model_thresholds (æ¨¡å‹ç‰¹å®šé˜ˆå€¼)
  é»˜è®¤: {}
  è¯´æ˜: é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„é˜ˆå€¼è¦†ç›–é…ç½®ã€‚
  ç¤ºä¾‹: {"gpt-4": {"compression_threshold_tokens": 8000, "max_context_tokens": 32000}}

keep_first (ä¿ç•™åˆå§‹æ¶ˆæ¯æ•°)
  é»˜è®¤: 1
  è¯´æ˜: å§‹ç»ˆä¿ç•™å¯¹è¯å¼€å§‹çš„ N æ¡æ¶ˆæ¯ã€‚è®¾ç½®ä¸º 0 åˆ™ä¸ä¿ç•™ã€‚ç¬¬ä¸€æ¡æ¶ˆæ¯é€šå¸¸åŒ…å«é‡è¦çš„æç¤ºæˆ–ç¯å¢ƒå˜é‡ã€‚

keep_last (ä¿ç•™æœ€è¿‘æ¶ˆæ¯æ•°)
  é»˜è®¤: 6
  è¯´æ˜: å§‹ç»ˆä¿ç•™å¯¹è¯æœ«å°¾çš„ N æ¡å®Œæ•´æ¶ˆæ¯ï¼Œä»¥ç¡®ä¿ä¸Šä¸‹æ–‡çš„è¿è´¯æ€§ã€‚

summary_model (æ‘˜è¦æ¨¡å‹)
  é»˜è®¤: None
  è¯´æ˜: ç”¨äºç”Ÿæˆæ‘˜è¦çš„ LLM æ¨¡å‹ã€‚
  å»ºè®®:
    - å¼ºçƒˆå»ºè®®é…ç½®ä¸€ä¸ªå¿«é€Ÿä¸”ç»æµçš„å…¼å®¹æ¨¡å‹ï¼Œå¦‚ `deepseek-v3`ã€`gemini-2.5-flash`ã€`gpt-4.1`ã€‚
    - å¦‚æœç•™ç©ºï¼Œè¿‡æ»¤å™¨å°†å°è¯•ä½¿ç”¨å½“å‰å¯¹è¯çš„æ¨¡å‹ã€‚
  æ³¨æ„:
    - å¦‚æœå½“å‰å¯¹è¯ä½¿ç”¨çš„æ˜¯æµæ°´çº¿ï¼ˆPipeï¼‰æ¨¡å‹æˆ–ä¸ç›´æ¥æ”¯æŒæ ‡å‡†ç”ŸæˆAPIçš„æ¨¡å‹ï¼Œç•™ç©ºæ­¤é¡¹å¯èƒ½ä¼šå¯¼è‡´æ‘˜è¦ç”Ÿæˆå¤±è´¥ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¿…é¡»æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆçš„æ¨¡å‹ã€‚

max_summary_tokens (æ‘˜è¦é•¿åº¦)
  é»˜è®¤: 16384
  è¯´æ˜: ç”Ÿæˆæ‘˜è¦æ—¶å…è®¸çš„æœ€å¤§ token æ•°ã€‚

summary_temperature (æ‘˜è¦æ¸©åº¦)
  é»˜è®¤: 0.3
  è¯´æ˜: æ§åˆ¶æ‘˜è¦ç”Ÿæˆçš„éšæœºæ€§ï¼Œè¾ƒä½çš„å€¼ä¼šäº§ç”Ÿæ›´ç¡®å®šæ€§çš„è¾“å‡ºã€‚

debug_mode (è°ƒè¯•æ¨¡å¼)
  é»˜è®¤: true
  è¯´æ˜: åœ¨æ—¥å¿—ä¸­æ‰“å°è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ã€‚ç”Ÿäº§ç¯å¢ƒå»ºè®®è®¾ä¸º `false`ã€‚

show_debug_log (å‰ç«¯è°ƒè¯•æ—¥å¿—)
  é»˜è®¤: false
  è¯´æ˜: åœ¨æµè§ˆå™¨æ§åˆ¶å°æ‰“å°è°ƒè¯•æ—¥å¿— (F12)ã€‚ä¾¿äºå‰ç«¯è°ƒè¯•ã€‚

ğŸ”§ éƒ¨ç½²é…ç½®
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ’ä»¶è‡ªåŠ¨ä½¿ç”¨ Open WebUI çš„å…±äº«æ•°æ®åº“è¿æ¥ã€‚
æ— éœ€é¢å¤–çš„æ•°æ®åº“é…ç½®ã€‚

è¿‡æ»¤å™¨å®‰è£…é¡ºåºå»ºè®®ï¼š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å»ºè®®å°†æ­¤è¿‡æ»¤å™¨çš„ä¼˜å…ˆçº§è®¾ç½®å¾—ç›¸å¯¹è¾ƒé«˜ï¼ˆæ•°å€¼è¾ƒå°ï¼‰ï¼Œä»¥ç¡®ä¿å®ƒåœ¨å…¶ä»–å¯èƒ½ä¿®æ”¹æ¶ˆæ¯å†…å®¹çš„è¿‡æ»¤å™¨ä¹‹å‰è¿è¡Œã€‚ä¸€ä¸ªå…¸å‹çš„é¡ºåºå¯èƒ½æ˜¯ï¼š

  1. éœ€è¦è®¿é—®å®Œæ•´ã€æœªå‹ç¼©å†å²è®°å½•çš„è¿‡æ»¤å™¨ (priority < 10)
     (ä¾‹å¦‚: æ³¨å…¥ç³»ç»Ÿçº§æç¤ºçš„è¿‡æ»¤å™¨)
  2. æœ¬å‹ç¼©è¿‡æ»¤å™¨ (priority = 10)
  3. åœ¨å‹ç¼©åè¿è¡Œçš„è¿‡æ»¤å™¨ (priority > 10)
     (ä¾‹å¦‚: æœ€ç»ˆè¾“å‡ºæ ¼å¼åŒ–è¿‡æ»¤å™¨)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ æ•°æ®åº“æŸ¥è¯¢ç¤ºä¾‹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æŸ¥çœ‹æ‰€æœ‰æ‘˜è¦ï¼š
  SELECT
    chat_id,
    LEFT(summary, 100) as summary_preview,
    compressed_message_count,
    updated_at
  FROM chat_summary
  ORDER BY updated_at DESC;

æŸ¥è¯¢ç‰¹å®šå¯¹è¯ï¼š
  SELECT *
  FROM chat_summary
  WHERE chat_id = 'your_chat_id';

åˆ é™¤è¿‡æœŸæ‘˜è¦ï¼š
  DELETE FROM chat_summary
  WHERE updated_at < NOW() - INTERVAL '30 days';

ç»Ÿè®¡ä¿¡æ¯ï¼š
  SELECT
    COUNT(*) as total_summaries,
    AVG(LENGTH(summary)) as avg_summary_length,
    AVG(compressed_message_count) as avg_msg_count
  FROM chat_summary;

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ æ³¨æ„äº‹é¡¹
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. æ•°æ®åº“è¿æ¥
   âœ“ æ’ä»¶è‡ªåŠ¨ä½¿ç”¨ Open WebUI çš„å…±äº«æ•°æ®åº“è¿æ¥ã€‚
   âœ“ æ— éœ€é¢å¤–é…ç½®ã€‚
   âœ“ é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨åˆ›å»º `chat_summary` è¡¨ã€‚

2. ä¿ç•™ç­–ç•¥
   âš  `keep_first` é…ç½®å¯¹äºä¿ç•™åŒ…å«æç¤ºæˆ–ç¯å¢ƒå˜é‡çš„åˆå§‹æ¶ˆæ¯éå¸¸é‡è¦ã€‚è¯·æ ¹æ®éœ€è¦è¿›è¡Œé…ç½®ã€‚

3. æ€§èƒ½è€ƒè™‘
   âš  æ‘˜è¦ç”Ÿæˆæ˜¯å¼‚æ­¥çš„ï¼Œä¸ä¼šé˜»å¡ç”¨æˆ·å“åº”ã€‚
   âš  é¦–æ¬¡è¾¾åˆ°é˜ˆå€¼æ—¶ä¼šæœ‰çŸ­æš‚çš„åå°å¤„ç†æ—¶é—´ã€‚

4. æˆæœ¬ä¼˜åŒ–
   âš  æ¯æ¬¡è¾¾åˆ°é˜ˆå€¼ä¼šè°ƒç”¨ä¸€æ¬¡æ‘˜è¦æ¨¡å‹ã€‚
   âš  åˆç†è®¾ç½® `compression_threshold_tokens` é¿å…é¢‘ç¹è°ƒç”¨ã€‚
   âš  å»ºè®®ä½¿ç”¨å¿«é€Ÿä¸”ç»æµçš„æ¨¡å‹ï¼ˆå¦‚ `gemini-flash`ï¼‰ç”Ÿæˆæ‘˜è¦ã€‚

5. å¤šæ¨¡æ€æ”¯æŒ
   âœ“ æœ¬è¿‡æ»¤å™¨æ”¯æŒåŒ…å«å›¾ç‰‡çš„å¤šæ¨¡æ€æ¶ˆæ¯ã€‚
   âœ“ æ‘˜è¦ä»…é’ˆå¯¹æ–‡æœ¬å†…å®¹ç”Ÿæˆã€‚
   âœ“ åœ¨å‹ç¼©è¿‡ç¨‹ä¸­ï¼Œéæ–‡æœ¬éƒ¨åˆ†ï¼ˆå¦‚å›¾ç‰‡ï¼‰ä¼šè¢«ä¿ç•™åœ¨åŸå§‹æ¶ˆæ¯ä¸­ã€‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ› æ•…éšœæ’é™¤
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

é—®é¢˜ï¼šæ•°æ®åº“è¡¨æœªåˆ›å»º
è§£å†³ï¼š
  1. ç¡®ä¿ Open WebUI å·²æ­£ç¡®é…ç½®æ•°æ®åº“ã€‚
  2. æŸ¥çœ‹ Open WebUI çš„å®¹å™¨æ—¥å¿—ä»¥è·å–è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ã€‚
  3. éªŒè¯ Open WebUI çš„æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

é—®é¢˜ï¼šæ‘˜è¦æœªç”Ÿæˆ
è§£å†³ï¼š
  1. æ£€æŸ¥æ˜¯å¦è¾¾åˆ° `compression_threshold_tokens`ã€‚
  2. æŸ¥çœ‹ `summary_model` æ˜¯å¦é…ç½®æ­£ç¡®ã€‚
  3. æ£€æŸ¥è°ƒè¯•æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯ã€‚

é—®é¢˜ï¼šåˆå§‹çš„æç¤ºæˆ–ç¯å¢ƒå˜é‡ä¸¢å¤±
è§£å†³ï¼š
  - ç¡®ä¿ `keep_first` è®¾ç½®ä¸ºå¤§äº 0 çš„å€¼ï¼Œä»¥ä¿ç•™åŒ…å«è¿™äº›ä¿¡æ¯çš„åˆå§‹æ¶ˆæ¯ã€‚

é—®é¢˜ï¼šå‹ç¼©æ•ˆæœä¸æ˜æ˜¾
è§£å†³ï¼š
  1. é€‚å½“æé«˜ `compression_threshold_tokens`ã€‚
  2. å‡å°‘ `keep_last` æˆ– `keep_first` çš„æ•°é‡ã€‚
  3. æ£€æŸ¥å¯¹è¯æ˜¯å¦çœŸçš„å¾ˆé•¿ã€‚


"""

from pydantic import BaseModel, Field, model_validator
from typing import Optional, Dict, Any, List, Union, Callable, Awaitable
import re
import asyncio
import json
import hashlib
import time
import contextlib

# Open WebUI å†…ç½®å¯¼å…¥
from open_webui.utils.chat import generate_chat_completion
from open_webui.models.users import Users
from open_webui.models.models import Models
from fastapi.requests import Request
from open_webui.main import app as webui_app

# Open WebUI å†…éƒ¨æ•°æ®åº“ (å¤ç”¨å…±äº«è¿æ¥)
try:
    from open_webui.internal import db as owui_db
except ModuleNotFoundError:  # pragma: no cover - filter runs inside Open WebUI
    owui_db = None

# å°è¯•å¯¼å…¥ tiktoken
try:
    import tiktoken
except ImportError:
    tiktoken = None

# æ•°æ®åº“å¯¼å…¥
from sqlalchemy import Column, String, Text, DateTime, Integer, inspect
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy.engine import Engine
from datetime import datetime


def _discover_owui_engine(db_module: Any) -> Optional[Engine]:
    """Discover the Open WebUI SQLAlchemy engine via provided db module helpers."""
    if db_module is None:
        return None

    db_context = getattr(db_module, "get_db_context", None) or getattr(
        db_module, "get_db", None
    )
    if callable(db_context):
        try:
            with db_context() as session:
                try:
                    return session.get_bind()
                except AttributeError:
                    return getattr(session, "bind", None) or getattr(
                        session, "engine", None
                    )
        except Exception as exc:
            print(f"[DB Discover] get_db_context failed: {exc}")

    for attr in ("engine", "ENGINE", "bind", "BIND"):
        candidate = getattr(db_module, attr, None)
        if candidate is not None:
            return candidate

    return None


def _discover_owui_schema(db_module: Any) -> Optional[str]:
    """Discover the Open WebUI database schema name if configured."""
    if db_module is None:
        return None

    try:
        base = getattr(db_module, "Base", None)
        metadata = getattr(base, "metadata", None) if base is not None else None
        candidate = getattr(metadata, "schema", None) if metadata is not None else None
        if isinstance(candidate, str) and candidate.strip():
            return candidate.strip()
    except Exception as exc:
        print(f"[DB Discover] Base metadata schema lookup failed: {exc}")

    try:
        metadata_obj = getattr(db_module, "metadata_obj", None)
        candidate = (
            getattr(metadata_obj, "schema", None) if metadata_obj is not None else None
        )
        if isinstance(candidate, str) and candidate.strip():
            return candidate.strip()
    except Exception as exc:
        print(f"[DB Discover] metadata_obj schema lookup failed: {exc}")

    try:
        from open_webui import env as owui_env

        candidate = getattr(owui_env, "DATABASE_SCHEMA", None)
        if isinstance(candidate, str) and candidate.strip():
            return candidate.strip()
    except Exception as exc:
        print(f"[DB Discover] env schema lookup failed: {exc}")

    return None


owui_engine = _discover_owui_engine(owui_db)
owui_schema = _discover_owui_schema(owui_db)
owui_Base = getattr(owui_db, "Base", None) if owui_db is not None else None
if owui_Base is None:
    owui_Base = declarative_base()


class ChatSummary(owui_Base):
    """å¯¹è¯æ‘˜è¦å­˜å‚¨è¡¨"""

    __tablename__ = "chat_summary"
    __table_args__ = (
        {"extend_existing": True, "schema": owui_schema}
        if owui_schema
        else {"extend_existing": True}
    )

    id = Column(Integer, primary_key=True, autoincrement=True)
    chat_id = Column(String(255), unique=True, nullable=False, index=True)
    summary = Column(Text, nullable=False)
    compressed_message_count = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)


class Filter:
    def __init__(self):
        self.valves = self.Valves()
        self._owui_db = owui_db
        self._db_engine = owui_engine
        self._fallback_session_factory = (
            sessionmaker(bind=self._db_engine) if self._db_engine else None
        )
        self._init_database()

    @contextlib.contextmanager
    def _db_session(self):
        """Yield a database session using Open WebUI helpers with graceful fallbacks."""
        db_module = self._owui_db
        db_context = None
        if db_module is not None:
            db_context = getattr(db_module, "get_db_context", None) or getattr(
                db_module, "get_db", None
            )

        if callable(db_context):
            with db_context() as session:
                yield session
                return

        factory = None
        if db_module is not None:
            factory = getattr(db_module, "SessionLocal", None) or getattr(
                db_module, "ScopedSession", None
            )
        if callable(factory):
            session = factory()
            try:
                yield session
            finally:
                close = getattr(session, "close", None)
                if callable(close):
                    close()
            return

        if self._fallback_session_factory is None:
            raise RuntimeError(
                "Open WebUI database session is unavailable. Ensure Open WebUI's database layer is initialized."
            )

        session = self._fallback_session_factory()
        try:
            yield session
        finally:
            try:
                session.close()
            except Exception as exc:  # pragma: no cover - best-effort cleanup
                print(f"[Database] âš ï¸ Failed to close fallback session: {exc}")

    def _init_database(self):
        """ä½¿ç”¨ Open WebUI çš„å…±äº«è¿æ¥åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        try:
            if self._db_engine is None:
                raise RuntimeError(
                    "Open WebUI database engine is unavailable. Ensure Open WebUI is configured with a valid DATABASE_URL."
                )

            # ä½¿ç”¨ SQLAlchemy inspect æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            inspector = inspect(self._db_engine)
            if not inspector.has_table("chat_summary"):
                # å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
                ChatSummary.__table__.create(bind=self._db_engine, checkfirst=True)
                print(
                    "[æ•°æ®åº“] âœ… ä½¿ç”¨ Open WebUI çš„å…±äº«æ•°æ®åº“è¿æ¥æˆåŠŸåˆ›å»º chat_summary è¡¨ã€‚"
                )
            else:
                print(
                    "[æ•°æ®åº“] âœ… ä½¿ç”¨ Open WebUI çš„å…±äº«æ•°æ®åº“è¿æ¥ã€‚chat_summary è¡¨å·²å­˜åœ¨ã€‚"
                )

        except Exception as e:
            print(f"[æ•°æ®åº“] âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    class Valves(BaseModel):
        priority: int = Field(
            default=10, description="Priority level for the filter operations."
        )
        # Token ç›¸å…³å‚æ•°
        compression_threshold_tokens: int = Field(
            default=64000,
            ge=0,
            description="å½“ä¸Šä¸‹æ–‡æ€» Token æ•°è¶…è¿‡æ­¤å€¼æ—¶ï¼Œè§¦å‘å‹ç¼© (å…¨å±€é»˜è®¤å€¼)",
        )
        max_context_tokens: int = Field(
            default=128000,
            ge=0,
            description="ä¸Šä¸‹æ–‡çš„ç¡¬æ€§ä¸Šé™ã€‚è¶…è¿‡æ­¤å€¼å°†å¼ºåˆ¶ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯ (å…¨å±€é»˜è®¤å€¼)",
        )
        model_thresholds: dict = Field(
            default={},
            description="é’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„é˜ˆå€¼è¦†ç›–é…ç½®ã€‚ä»…åŒ…å«éœ€è¦ç‰¹æ®Šé…ç½®çš„æ¨¡å‹ã€‚",
        )

        keep_first: int = Field(
            default=1, ge=0, description="å§‹ç»ˆä¿ç•™æœ€åˆçš„ N æ¡æ¶ˆæ¯ã€‚è®¾ç½®ä¸º 0 åˆ™ä¸ä¿ç•™ã€‚"
        )
        keep_last: int = Field(
            default=6, ge=0, description="å§‹ç»ˆä¿ç•™æœ€è¿‘çš„ N æ¡å®Œæ•´æ¶ˆæ¯ã€‚"
        )
        summary_model: str = Field(
            default=None,
            description="ç”¨äºç”Ÿæˆæ‘˜è¦çš„æ¨¡å‹ IDã€‚ç•™ç©ºåˆ™ä½¿ç”¨å½“å‰å¯¹è¯çš„æ¨¡å‹ã€‚ç”¨äºåŒ¹é… model_thresholds ä¸­çš„é…ç½®ã€‚",
        )
        max_summary_tokens: int = Field(
            default=16384, ge=1, description="æ‘˜è¦çš„æœ€å¤§ token æ•°"
        )
        summary_temperature: float = Field(
            default=0.1, ge=0.0, le=2.0, description="æ‘˜è¦ç”Ÿæˆçš„æ¸©åº¦å‚æ•°"
        )
        debug_mode: bool = Field(default=True, description="è°ƒè¯•æ¨¡å¼ï¼Œæ‰“å°è¯¦ç»†æ—¥å¿—")
        show_debug_log: bool = Field(
            default=False, description="åœ¨æµè§ˆå™¨æ§åˆ¶å°æ‰“å°è°ƒè¯•æ—¥å¿— (F12)"
        )
        show_token_usage_status: bool = Field(
            default=True, description="åœ¨å¯¹è¯ç»“æŸæ—¶æ˜¾ç¤º Token ä½¿ç”¨æƒ…å†µçš„çŠ¶æ€é€šçŸ¥"
        )
        enable_tool_output_trimming: bool = Field(
            default=False,
            description="å¯ç”¨åŸç”Ÿå·¥å…·è¾“å‡ºè£å‰ª (ä»…é€‚ç”¨äº native function calling)ï¼Œè£å‰ªè¿‡é•¿çš„å·¥å…·è¾“å‡ºä»¥èŠ‚çœ Tokenã€‚",
        )

    def _save_summary(self, chat_id: str, summary: str, compressed_count: int):
        """ä¿å­˜æ‘˜è¦åˆ°æ•°æ®åº“"""
        try:
            with self._db_session() as session:
                # æŸ¥æ‰¾ç°æœ‰è®°å½•
                existing = session.query(ChatSummary).filter_by(chat_id=chat_id).first()

                if existing:
                    # [ä¼˜åŒ–] ä¹è§‚é”æ£€æŸ¥ï¼šåªæœ‰è¿›åº¦å‘å‰æ¨è¿›æ—¶æ‰æ›´æ–°
                    if compressed_count <= existing.compressed_message_count:
                        if self.valves.debug_mode:
                            print(
                                f"[å­˜å‚¨] è·³è¿‡æ›´æ–°ï¼šæ–°è¿›åº¦ ({compressed_count}) ä¸å¤§äºç°æœ‰è¿›åº¦ ({existing.compressed_message_count})"
                            )
                        return

                    # æ›´æ–°ç°æœ‰è®°å½•
                    existing.summary = summary
                    existing.compressed_message_count = compressed_count
                    existing.updated_at = datetime.utcnow()
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_summary = ChatSummary(
                        chat_id=chat_id,
                        summary=summary,
                        compressed_message_count=compressed_count,
                    )
                    session.add(new_summary)

                session.commit()

                if self.valves.debug_mode:
                    action = "æ›´æ–°" if existing else "åˆ›å»º"
                    print(f"[å­˜å‚¨] æ‘˜è¦å·²{action}åˆ°æ•°æ®åº“ (Chat ID: {chat_id})")

        except Exception as e:
            print(f"[å­˜å‚¨] âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {str(e)}")

    def _load_summary_record(self, chat_id: str) -> Optional[ChatSummary]:
        """ä»æ•°æ®åº“åŠ è½½æ‘˜è¦è®°å½•å¯¹è±¡"""
        try:
            with self._db_session() as session:
                record = session.query(ChatSummary).filter_by(chat_id=chat_id).first()
                if record:
                    # Detach the object from the session so it can be used after session close
                    session.expunge(record)
                    return record
        except Exception as e:
            print(f"[åŠ è½½] âŒ æ•°æ®åº“è¯»å–å¤±è´¥: {str(e)}")
        return None

    def _load_summary(self, chat_id: str, body: dict) -> Optional[str]:
        """ä»æ•°æ®åº“åŠ è½½æ‘˜è¦æ–‡æœ¬ (å…¼å®¹æ—§æ¥å£)"""
        record = self._load_summary_record(chat_id)
        if record:
            if self.valves.debug_mode:
                print(f"[åŠ è½½] ä»æ•°æ®åº“åŠ è½½æ‘˜è¦ (Chat ID: {chat_id})")
                print(
                    f"[åŠ è½½] æ›´æ–°æ—¶é—´: {record.updated_at}, å·²å‹ç¼©æ¶ˆæ¯æ•°: {record.compressed_message_count}"
                )
            return record.summary
        return None

    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡"""
        if not text:
            return 0

        if tiktoken:
            try:
                # ç»Ÿä¸€ä½¿ç”¨ o200k_base ç¼–ç  (é€‚é…æœ€æ–°æ¨¡å‹)
                encoding = tiktoken.get_encoding("o200k_base")
                return len(encoding.encode(text))
            except Exception as e:
                if self.valves.debug_mode:
                    print(f"[Tokenè®¡æ•°] tiktoken é”™è¯¯: {e}ï¼Œå›é€€åˆ°å­—ç¬¦ä¼°ç®—")

        # å›é€€ç­–ç•¥ï¼šç²—ç•¥ä¼°ç®— (1 token â‰ˆ 4 chars)
        return len(text) // 4

    def _calculate_messages_tokens(self, messages: List[Dict]) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„æ€» Token æ•°"""
        total_tokens = 0
        for msg in messages:
            content = msg.get("content", "")
            if isinstance(content, list):
                # å¤šæ¨¡æ€å†…å®¹å¤„ç†
                text_content = ""
                for part in content:
                    if isinstance(part, dict) and part.get("type") == "text":
                        text_content += part.get("text", "")
                total_tokens += self._count_tokens(text_content)
            else:
                total_tokens += self._count_tokens(str(content))
        return total_tokens

    def _get_model_thresholds(self, model_id: str) -> Dict[str, int]:
        """è·å–ç‰¹å®šæ¨¡å‹çš„é˜ˆå€¼é…ç½®

        ä¼˜å…ˆçº§ï¼š
        1. å¦‚æœ model_thresholds ä¸­å­˜åœ¨è¯¥æ¨¡å‹IDçš„é…ç½®ï¼Œä½¿ç”¨è¯¥é…ç½®
        2. å¦åˆ™ä½¿ç”¨å…¨å±€å‚æ•° compression_threshold_tokens å’Œ max_context_tokens
        """
        # å°è¯•ä»æ¨¡å‹ç‰¹å®šé…ç½®ä¸­åŒ¹é…
        if model_id in self.valves.model_thresholds:
            if self.valves.debug_mode:
                print(f"[é…ç½®] ä½¿ç”¨æ¨¡å‹ç‰¹å®šé…ç½®: {model_id}")
            return self.valves.model_thresholds[model_id]

        # ä½¿ç”¨å…¨å±€é»˜è®¤é…ç½®
        if self.valves.debug_mode:
            print(f"[é…ç½®] æ¨¡å‹ {model_id} æœªåœ¨ model_thresholds ä¸­ï¼Œä½¿ç”¨å…¨å±€å‚æ•°")

        return {
            "compression_threshold_tokens": self.valves.compression_threshold_tokens,
            "max_context_tokens": self.valves.max_context_tokens,
        }

    def _get_chat_context(
        self, body: dict, __metadata__: Optional[dict] = None
    ) -> Dict[str, str]:
        """
        ç»Ÿä¸€æå–èŠå¤©ä¸Šä¸‹æ–‡ä¿¡æ¯ (chat_id, message_id)ã€‚
        ä¼˜å…ˆä» body ä¸­æå–ï¼Œå…¶æ¬¡ä» metadata ä¸­æå–ã€‚
        """
        chat_id = ""
        message_id = ""

        # 1. å°è¯•ä» body è·å–
        if isinstance(body, dict):
            chat_id = body.get("chat_id", "")
            message_id = body.get("id", "")  # message_id åœ¨ body ä¸­é€šå¸¸æ˜¯ id

            # å†æ¬¡æ£€æŸ¥ body.metadata
            if not chat_id or not message_id:
                body_metadata = body.get("metadata", {})
                if isinstance(body_metadata, dict):
                    if not chat_id:
                        chat_id = body_metadata.get("chat_id", "")
                    if not message_id:
                        message_id = body_metadata.get("message_id", "")

        # 2. å°è¯•ä» __metadata__ è·å– (ä½œä¸ºè¡¥å……)
        if __metadata__ and isinstance(__metadata__, dict):
            if not chat_id:
                chat_id = __metadata__.get("chat_id", "")
            if not message_id:
                message_id = __metadata__.get("message_id", "")

        return {
            "chat_id": str(chat_id).strip(),
            "message_id": str(message_id).strip(),
        }

    async def _emit_debug_log(
        self,
        __event_call__,
        chat_id: str,
        original_count: int,
        compressed_count: int,
        summary_length: int,
        kept_first: int,
        kept_last: int,
    ):
        """Emit debug log to browser console via JS execution"""
        if not self.valves.show_debug_log or not __event_call__:
            return

        try:
            # Prepare data for JS
            log_data = {
                "chatId": chat_id,
                "originalCount": original_count,
                "compressedCount": compressed_count,
                "summaryLength": summary_length,
                "keptFirst": kept_first,
                "keptLast": kept_last,
                "ratio": (
                    f"{(1 - compressed_count/original_count)*100:.1f}%"
                    if original_count > 0
                    else "0%"
                ),
            }

            # Construct JS code
            js_code = f"""
                (async function() {{
                    console.group("ğŸ—œï¸ Async Context Compression Debug");
                    console.log("Chat ID:", {json.dumps(chat_id)});
                    console.log("Messages:", {original_count} + " -> " + {compressed_count});
                    console.log("Compression Ratio:", {json.dumps(log_data['ratio'])});
                    console.log("Summary Length:", {summary_length} + " chars");
                    console.log("Configuration:", {{
                        "Keep First": {kept_first},
                        "Keep Last": {kept_last}
                    }});
                    console.groupEnd();
                }})();
            """

            await __event_call__(
                {
                    "type": "execute",
                    "data": {"code": js_code},
                }
            )
        except Exception as e:
            print(f"Error emitting debug log: {e}")

    async def _log(self, message: str, type: str = "info", event_call=None):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡ºåˆ°åç«¯ (print) å’Œå‰ç«¯ (console.log)"""
        # åç«¯æ—¥å¿—
        if self.valves.debug_mode:
            print(message)

        # å‰ç«¯æ—¥å¿—
        if self.valves.show_debug_log and event_call:
            try:
                css = "color: #3b82f6;"  # é»˜è®¤è“è‰²
                if type == "error":
                    css = "color: #ef4444; font-weight: bold;"  # çº¢è‰²
                elif type == "warning":
                    css = "color: #f59e0b;"  # æ©™è‰²
                elif type == "success":
                    css = "color: #10b981; font-weight: bold;"  # ç»¿è‰²

                # æ¸…ç†å‰ç«¯æ¶ˆæ¯ï¼šç§»é™¤åˆ†éš”ç¬¦å’Œå¤šä½™æ¢è¡Œ
                lines = message.split("\n")
                # ä¿ç•™ä¸ä»¥å¤§é‡ç­‰å·æˆ–è¿å­—ç¬¦å¼€å¤´çš„è¡Œ
                filtered_lines = [
                    line
                    for line in lines
                    if not line.strip().startswith("====")
                    and not line.strip().startswith("----")
                ]
                clean_message = "\n".join(filtered_lines).strip()

                if not clean_message:
                    return

                # è½¬ä¹‰æ¶ˆæ¯ä¸­çš„å¼•å·å’Œæ¢è¡Œç¬¦
                safe_message = clean_message.replace('"', '\\"').replace("\n", "\\n")

                js_code = f"""
                    console.log("%c[å‹ç¼©] {safe_message}", "{css}");
                """
                await event_call({"type": "execute", "data": {"code": js_code}})
            except Exception as e:
                print(f"å‘é€å‰ç«¯æ—¥å¿—å¤±è´¥: {e}")

    async def inlet(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __metadata__: dict = None,
        __request__: Request = None,
        __model__: dict = None,
        __event_emitter__: Callable[[Any], Awaitable[None]] = None,
        __event_call__: Callable[[Any], Awaitable[None]] = None,
    ) -> dict:
        """
        åœ¨å‘é€åˆ° LLM ä¹‹å‰æ‰§è¡Œ
        å‹ç¼©ç­–ç•¥ï¼š
        1. æ³¨å…¥å·²æœ‰æ‘˜è¦
        2. é¢„æ£€ Token é¢„ç®—
        3. å¦‚æœè¶…é™ï¼Œæ‰§è¡Œç»“æ„åŒ–è£å‰ªï¼ˆStructure-Aware Trimmingï¼‰æˆ–ä¸¢å¼ƒæ—§æ¶ˆæ¯
        """
        messages = body.get("messages", [])

        # --- åŸç”Ÿå·¥å…·è¾“å‡ºè£å‰ª (Native Tool Output Trimming) ---
        metadata = body.get("metadata", {})
        is_native_func_calling = metadata.get("function_calling") == "native"

        if self.valves.enable_tool_output_trimming and is_native_func_calling:
            trimmed_count = 0
            for msg in messages:
                content = msg.get("content", "")
                if not isinstance(content, str):
                    continue

                role = msg.get("role")

                # ä»…å¤„ç†å¸¦æœ‰åŸç”Ÿå·¥å…·è¾“å‡ºçš„åŠ©æ‰‹æ¶ˆæ¯
                if role == "assistant":
                    # æ£€æµ‹åŠ©æ‰‹å†…å®¹ä¸­çš„å·¥å…·è¾“å‡ºæ ‡è®°
                    if "tool_call_id:" in content or (
                        content.startswith('"') and "\\&quot;" in content
                    ):
                        if self.valves.show_debug_log and __event_call__:
                            await self._log(
                                f"[Inlet] ğŸ” æ£€æµ‹åˆ°åŠ©æ‰‹æ¶ˆæ¯ä¸­çš„åŸç”Ÿå·¥å…·è¾“å‡ºã€‚",
                                event_call=__event_call__,
                            )

                        # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆåœ¨æœ€åä¸€ä¸ªå·¥å…·è°ƒç”¨å…ƒæ•°æ®ä¹‹åï¼‰
                        # æ¨¡å¼ï¼šåŒ¹é…è½¬ä¹‰çš„ JSON å­—ç¬¦ä¸²ï¼Œå¦‚ ""&quot;...&quot;"" åè·Ÿæ¢è¡Œç¬¦
                        # æˆ‘ä»¬å¯»æ‰¾è¯¥æ¨¡å¼çš„æœ€åä¸€æ¬¡å‡ºç°ï¼Œå¹¶è·å–å…¶åçš„æ‰€æœ‰å†…å®¹

                        # 1. å°è¯•åŒ¹é…ç‰¹å®šçš„ OpenWebUI å·¥å…·è¾“å‡ºæ ¼å¼ï¼š""&quot;...&quot;""
                        tool_output_pattern = r'""&quot;.*?&quot;""\s*'

                        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
                        matches = list(
                            re.finditer(tool_output_pattern, content, re.DOTALL)
                        )

                        if matches:
                            # è·å–æœ€åä¸€ä¸ªåŒ¹é…é¡¹çš„ç»“æŸä½ç½®
                            last_match_end = matches[-1].end()

                            # æœ€åä¸€ä¸ªå·¥å…·è¾“å‡ºä¹‹åçš„æ‰€æœ‰å†…å®¹å³ä¸ºæœ€ç»ˆç­”æ¡ˆ
                            final_answer = content[last_match_end:].strip()

                            if final_answer:
                                msg["content"] = (
                                    f"... [Tool outputs trimmed]\n{final_answer}"
                                )
                                trimmed_count += 1
                        else:
                            # å›é€€ï¼šå¦‚æœæ‰¾ä¸åˆ°æ–°æ ¼å¼ï¼Œå°è¯•æŒ‰ "Arguments:" åˆ†å‰²
                            # (ä¿ç•™å‘åå…¼å®¹æ€§æˆ–é€‚åº”ä¸åŒæ¨¡å‹è¡Œä¸º)
                            parts = re.split(r"(?:Arguments:\s*\{[^}]+\})\n+", content)
                            if len(parts) > 1:
                                final_answer = parts[-1].strip()
                                if final_answer:
                                    msg["content"] = (
                                        f"... [Tool outputs trimmed]\n{final_answer}"
                                    )
                                    trimmed_count += 1

            if trimmed_count > 0 and self.valves.show_debug_log and __event_call__:
                await self._log(
                    f"[Inlet] âœ‚ï¸ å·²è£å‰ª {trimmed_count} æ¡å·¥å…·è¾“å‡ºæ¶ˆæ¯ã€‚",
                    event_call=__event_call__,
                )

        chat_ctx = self._get_chat_context(body, __metadata__)
        chat_id = chat_ctx["chat_id"]

        # æå–ç³»ç»Ÿæç¤ºè¯ä»¥è¿›è¡Œå‡†ç¡®çš„ Token è®¡ç®—
        # 1. å¯¹äºè‡ªå®šä¹‰æ¨¡å‹ï¼šæ£€æŸ¥æ•°æ®åº“ (Models.get_model_by_id)
        # 2. å¯¹äºåŸºç¡€æ¨¡å‹ï¼šæ£€æŸ¥æ¶ˆæ¯ä¸­çš„ role='system'
        system_prompt_content = None

        # å°è¯•ä»æ•°æ®åº“è·å– (è‡ªå®šä¹‰æ¨¡å‹)
        try:
            model_id = body.get("model")
            if model_id:
                if self.valves.show_debug_log and __event_call__:
                    await self._log(
                        f"[Inlet] ğŸ” å°è¯•ä»æ•°æ®åº“æŸ¥æ‰¾æ¨¡å‹: {model_id}",
                        event_call=__event_call__,
                    )

                # æ¸…ç†æ¨¡å‹ ID
                model_obj = Models.get_model_by_id(model_id)

                if model_obj:
                    if self.valves.show_debug_log and __event_call__:
                        await self._log(
                            f"[Inlet] âœ… æ•°æ®åº“ä¸­æ‰¾åˆ°æ¨¡å‹: {model_obj.name} (ID: {model_obj.id})",
                            event_call=__event_call__,
                        )

                    if model_obj.params:
                        try:
                            params = model_obj.params
                            # å¤„ç† params æ˜¯ JSON å­—ç¬¦ä¸²çš„æƒ…å†µ
                            if isinstance(params, str):
                                params = json.loads(params)

                            # å¤„ç†å­—å…¸æˆ– Pydantic å¯¹è±¡
                            if isinstance(params, dict):
                                system_prompt_content = params.get("system")
                            else:
                                # å‡è®¾æ˜¯ Pydantic æ¨¡å‹æˆ–å¯¹è±¡
                                system_prompt_content = getattr(params, "system", None)

                            if system_prompt_content:
                                if self.valves.show_debug_log and __event_call__:
                                    await self._log(
                                        f"[Inlet] ğŸ“ åœ¨æ•°æ®åº“å‚æ•°ä¸­æ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯ ({len(system_prompt_content)} å­—ç¬¦)",
                                        event_call=__event_call__,
                                    )
                            else:
                                if self.valves.show_debug_log and __event_call__:
                                    await self._log(
                                        f"[Inlet] âš ï¸ æ¨¡å‹å‚æ•°ä¸­ç¼ºå°‘ 'system' é”®",
                                        event_call=__event_call__,
                                    )
                        except Exception as e:
                            if self.valves.show_debug_log and __event_call__:
                                await self._log(
                                    f"[Inlet] âŒ è§£ææ¨¡å‹å‚æ•°å¤±è´¥: {e}",
                                    type="error",
                                    event_call=__event_call__,
                                )

                    else:
                        if self.valves.show_debug_log and __event_call__:
                            await self._log(
                                f"[Inlet] âš ï¸ æ¨¡å‹å‚æ•°ä¸ºç©º",
                                event_call=__event_call__,
                            )
                else:
                    if self.valves.show_debug_log and __event_call__:
                        await self._log(
                            f"[Inlet] âŒ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹",
                            type="warning",
                            event_call=__event_call__,
                        )

        except Exception as e:
            if self.valves.show_debug_log and __event_call__:
                await self._log(
                    f"[Inlet] âŒ ä»æ•°æ®åº“è·å–ç³»ç»Ÿæç¤ºè¯é”™è¯¯: {e}",
                    type="error",
                    event_call=__event_call__,
                )
            if self.valves.debug_mode:
                print(f"[Inlet] ä»æ•°æ®åº“è·å–ç³»ç»Ÿæç¤ºè¯é”™è¯¯: {e}")

        # å›é€€ï¼šæ£€æŸ¥æ¶ˆæ¯åˆ—è¡¨ (åŸºç¡€æ¨¡å‹æˆ–å·²åŒ…å«)
        if not system_prompt_content:
            for msg in messages:
                if msg.get("role") == "system":
                    system_prompt_content = msg.get("content", "")
                    break

        # æ„å»º system_prompt_msg ç”¨äº Token è®¡ç®—
        system_prompt_msg = None
        if system_prompt_content:
            system_prompt_msg = {"role": "system", "content": system_prompt_content}
            if self.valves.debug_mode:
                print(
                    f"[Inlet] æ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯ ({len(system_prompt_content)} å­—ç¬¦)ã€‚è®¡å…¥é¢„ç®—ã€‚"
                )

        # è®°å½•æ¶ˆæ¯ç»Ÿè®¡ä¿¡æ¯ (ç§»è‡³æ­¤å¤„ä»¥åŒ…å«æå–çš„ç³»ç»Ÿæç¤ºè¯)
        if self.valves.show_debug_log and __event_call__:
            try:
                msg_stats = {
                    "user": 0,
                    "assistant": 0,
                    "system": 0,
                    "total": len(messages),
                }
                for msg in messages:
                    role = msg.get("role", "unknown")
                    if role in msg_stats:
                        msg_stats[role] += 1

                # å¦‚æœç³»ç»Ÿæç¤ºè¯æ˜¯ä» DB/Model æå–çš„ä½†ä¸åœ¨æ¶ˆæ¯ä¸­ï¼Œåˆ™è®¡æ•°
                if system_prompt_content:
                    # æ£€æŸ¥æ˜¯å¦å·²è®¡æ•° (å³æ˜¯å¦åœ¨æ¶ˆæ¯ä¸­)
                    is_in_messages = any(m.get("role") == "system" for m in messages)
                    if not is_in_messages:
                        msg_stats["system"] += 1
                        msg_stats["total"] += 1

                stats_str = f"Total: {msg_stats['total']} | User: {msg_stats['user']} | Assistant: {msg_stats['assistant']} | System: {msg_stats['system']}"
                await self._log(
                    f"[Inlet] æ¶ˆæ¯ç»Ÿè®¡: {stats_str}", event_call=__event_call__
                )
            except Exception as e:
                print(f"[Inlet] è®°å½•æ¶ˆæ¯ç»Ÿè®¡é”™è¯¯: {e}")

        if not chat_id:
            await self._log(
                "[Inlet] âŒ metadata ä¸­ç¼ºå°‘ chat_idï¼Œè·³è¿‡å‹ç¼©",
                type="error",
                event_call=__event_call__,
            )
            return body

        if self.valves.debug_mode or self.valves.show_debug_log:
            await self._log(
                f"\n{'='*60}\n[Inlet] Chat ID: {chat_id}\n[Inlet] æ”¶åˆ° {len(messages)} æ¡æ¶ˆæ¯",
                event_call=__event_call__,
            )

        # è®°å½•åŸå§‹æ¶ˆæ¯çš„ç›®æ ‡å‹ç¼©è¿›åº¦ï¼Œä¾› outlet ä½¿ç”¨
        # ç›®æ ‡æ˜¯å‹ç¼©åˆ°å€’æ•°ç¬¬ keep_last æ¡ä¹‹å‰
        target_compressed_count = max(0, len(messages) - self.valves.keep_last)

        await self._log(
            f"[Inlet] è®°å½•ç›®æ ‡å‹ç¼©è¿›åº¦: {target_compressed_count}",
            event_call=__event_call__,
        )

        # åŠ è½½æ‘˜è¦è®°å½•
        summary_record = await asyncio.to_thread(self._load_summary_record, chat_id)

        # è®¡ç®— effective_keep_first ä»¥ç¡®ä¿æ‰€æœ‰ç³»ç»Ÿæ¶ˆæ¯éƒ½è¢«ä¿æŠ¤
        last_system_index = -1
        for i, msg in enumerate(messages):
            if msg.get("role") == "system":
                last_system_index = i

        effective_keep_first = max(self.valves.keep_first, last_system_index + 1)

        final_messages = []

        if summary_record:
            # å­˜åœ¨æ‘˜è¦ï¼Œæ„å»ºè§†å›¾ï¼š[Head] + [Summary Message] + [Tail]
            # Tail æ˜¯ä»ä¸Šæ¬¡å‹ç¼©ç‚¹ä¹‹åçš„æ‰€æœ‰æ¶ˆæ¯
            compressed_count = summary_record.compressed_message_count

            # ç¡®ä¿ compressed_count åˆç†
            if compressed_count > len(messages):
                compressed_count = max(0, len(messages) - self.valves.keep_last)

            # 1. å¤´éƒ¨æ¶ˆæ¯ (Keep First)
            head_messages = []
            if effective_keep_first > 0:
                head_messages = messages[:effective_keep_first]

            # 2. æ‘˜è¦æ¶ˆæ¯ (ä½œä¸º User æ¶ˆæ¯æ’å…¥)
            summary_content = (
                f"ã€ç³»ç»Ÿæç¤ºï¼šä»¥ä¸‹æ˜¯å†å²å¯¹è¯çš„æ‘˜è¦ï¼Œä»…ä¾›å‚è€ƒä¸Šä¸‹æ–‡ï¼Œè¯·å‹¿å¯¹æ‘˜è¦å†…å®¹è¿›è¡Œå›å¤ï¼Œç›´æ¥å›ç­”åç»­çš„æœ€æ–°é—®é¢˜ã€‘\n\n"
                f"{summary_record.summary}\n\n"
                f"---\n"
                f"ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯ï¼š"
            )
            summary_msg = {"role": "assistant", "content": summary_content}

            # 3. å°¾éƒ¨æ¶ˆæ¯ (Tail) - ä»ä¸Šæ¬¡å‹ç¼©ç‚¹å¼€å§‹çš„æ‰€æœ‰æ¶ˆæ¯
            # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ç¡®ä¿ä¸é‡å¤åŒ…å«å¤´éƒ¨æ¶ˆæ¯
            start_index = max(compressed_count, effective_keep_first)
            tail_messages = messages[start_index:]

            if self.valves.show_debug_log and __event_call__:
                tail_preview = [
                    f"{i + start_index}: [{m.get('role')}] {m.get('content', '')[:30]}..."
                    for i, m in enumerate(tail_messages)
                ]
                await self._log(
                    f"[Inlet] ğŸ“œ å°¾éƒ¨æ¶ˆæ¯ (èµ·å§‹ç´¢å¼•: {start_index}): {tail_preview}",
                    event_call=__event_call__,
                )

            # --- é¢„æ£€æ£€æŸ¥ä¸é¢„ç®— (Preflight Check & Budgeting) ---

            # ç»„è£…å€™é€‰æ¶ˆæ¯ (ç”¨äºè¾“å‡º)
            candidate_messages = head_messages + [summary_msg] + tail_messages

            # å‡†å¤‡ç”¨äº Token è®¡ç®—çš„æ¶ˆæ¯ (å¦‚æœç¼ºå°‘åˆ™åŒ…å«ç³»ç»Ÿæç¤ºè¯)
            calc_messages = candidate_messages
            if system_prompt_msg:
                # æ£€æŸ¥ç³»ç»Ÿæç¤ºè¯æ˜¯å¦å·²åœ¨ head_messages ä¸­
                is_in_head = any(m.get("role") == "system" for m in head_messages)
                if not is_in_head:
                    calc_messages = [system_prompt_msg] + candidate_messages

            # è·å–æœ€å¤§ä¸Šä¸‹æ–‡é™åˆ¶
            model = self._clean_model_id(body.get("model"))
            thresholds = self._get_model_thresholds(model)
            max_context_tokens = thresholds.get(
                "max_context_tokens", self.valves.max_context_tokens
            )

            # è®¡ç®—æ€» Token
            total_tokens = await asyncio.to_thread(
                self._calculate_messages_tokens, calc_messages
            )

            # é¢„æ£€æ£€æŸ¥æ—¥å¿—
            await self._log(
                f"[Inlet] ğŸ” é¢„æ£€æ£€æŸ¥: {total_tokens}t / {max_context_tokens}t ({(total_tokens/max_context_tokens*100):.1f}%)",
                event_call=__event_call__,
            )

            # å¦‚æœè¶…å‡ºé¢„ç®—ï¼Œç¼©å‡å†å²è®°å½• (Keep Last)
            if total_tokens > max_context_tokens:
                await self._log(
                    f"[Inlet] âš ï¸ å€™é€‰æç¤ºè¯ ({total_tokens} Tokens) è¶…è¿‡ä¸Šé™ ({max_context_tokens})ã€‚æ­£åœ¨ç¼©å‡å†å²è®°å½•...",
                    type="warning",
                    event_call=__event_call__,
                )

                # åŠ¨æ€ä» tail_messages çš„å¼€å¤´ç§»é™¤æ¶ˆæ¯
                # å§‹ç»ˆå°è¯•ä¿ç•™è‡³å°‘æœ€åä¸€æ¡æ¶ˆæ¯ (é€šå¸¸æ˜¯ç”¨æˆ·è¾“å…¥)
                while total_tokens > max_context_tokens and len(tail_messages) > 1:
                    # ç­–ç•¥ 1: ç»“æ„åŒ–åŠ©æ‰‹æ¶ˆæ¯è£å‰ª (Structure-Aware Assistant Trimming)
                    # ä¿ç•™: æ ‡é¢˜ (#), ç¬¬ä¸€è¡Œ, æœ€åä¸€è¡Œã€‚æŠ˜å å…¶ä½™éƒ¨åˆ†ã€‚
                    target_msg = None
                    target_idx = -1

                    # æŸ¥æ‰¾æœ€æ—§çš„ã€è¾ƒé•¿ä¸”å°šæœªè£å‰ªçš„åŠ©æ‰‹æ¶ˆæ¯
                    for i, msg in enumerate(tail_messages):
                        # è·³è¿‡æœ€åä¸€æ¡æ¶ˆæ¯ (é€šå¸¸æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œä¿æŠ¤å®ƒ)
                        if i == len(tail_messages) - 1:
                            break

                        if msg.get("role") == "assistant":
                            content = str(msg.get("content", ""))
                            is_trimmed = msg.get("metadata", {}).get(
                                "is_trimmed", False
                            )
                            # ä»…é’ˆå¯¹ç›¸å½“é•¿ (> 200 å­—ç¬¦) çš„æ¶ˆæ¯
                            if len(content) > 200 and not is_trimmed:
                                target_msg = msg
                                target_idx = i
                                break

                    # å¦‚æœæ‰¾åˆ°åˆé€‚çš„åŠ©æ‰‹æ¶ˆæ¯ï¼Œåº”ç”¨ç»“æ„åŒ–è£å‰ª
                    if target_msg:
                        content = str(target_msg.get("content", ""))
                        lines = content.split("\n")
                        kept_lines = []

                        # é€»è¾‘: ä¿ç•™æ ‡é¢˜, ç¬¬ä¸€è¡Œéç©ºè¡Œ, æœ€åä¸€è¡Œéç©ºè¡Œ
                        first_line_found = False
                        last_line_idx = -1

                        # æŸ¥æ‰¾æœ€åä¸€è¡Œéç©ºè¡Œçš„ç´¢å¼•
                        for idx in range(len(lines) - 1, -1, -1):
                            if lines[idx].strip():
                                last_line_idx = idx
                                break

                        for idx, line in enumerate(lines):
                            stripped = line.strip()
                            if not stripped:
                                continue

                            # ä¿ç•™æ ‡é¢˜ (H1-H6, éœ€è¦ # åæœ‰ç©ºæ ¼)
                            if re.match(r"^#{1,6}\s+", stripped):
                                kept_lines.append(line)
                                continue

                            # ä¿ç•™ç¬¬ä¸€è¡Œéç©ºè¡Œ
                            if not first_line_found:
                                kept_lines.append(line)
                                first_line_found = True
                                # å¦‚æœåé¢è¿˜æœ‰å†…å®¹ï¼Œæ·»åŠ å ä½ç¬¦
                                if idx < last_line_idx:
                                    kept_lines.append("\n... [Content collapsed] ...\n")
                                continue

                            # ä¿ç•™æœ€åä¸€è¡Œéç©ºè¡Œ
                            if idx == last_line_idx:
                                kept_lines.append(line)
                                continue

                        # æ›´æ–°æ¶ˆæ¯å†…å®¹
                        new_content = "\n".join(kept_lines)

                        # å®‰å…¨æ£€æŸ¥: å¦‚æœè£å‰ªæ²¡æœ‰èŠ‚çœå¤ªå¤š (ä¾‹å¦‚ä¸»è¦æ˜¯æ ‡é¢˜)ï¼Œåˆ™å¼ºåˆ¶ä¸¢å¼ƒ
                        if len(new_content) > len(content) * 0.8:
                            # å¦‚æœç»“æ„ä¿ç•™è¿‡äºå†—é•¿ï¼Œå›é€€åˆ°ä¸¢å¼ƒ
                            pass
                        else:
                            target_msg["content"] = new_content
                            if "metadata" not in target_msg:
                                target_msg["metadata"] = {}
                            target_msg["metadata"]["is_trimmed"] = True

                            # è®¡ç®— Token å‡å°‘é‡
                            old_tokens = self._count_tokens(content)
                            new_tokens = self._count_tokens(target_msg["content"])
                            diff = old_tokens - new_tokens
                            total_tokens -= diff

                            if self.valves.show_debug_log and __event_call__:
                                await self._log(
                                    f"[Inlet] ğŸ“‰ ç»“æ„åŒ–è£å‰ªåŠ©æ‰‹æ¶ˆæ¯ã€‚èŠ‚çœ: {diff} tokensã€‚",
                                    event_call=__event_call__,
                                )
                            continue

                    # ç­–ç•¥ 2: å›é€€ - å®Œå…¨ä¸¢å¼ƒæœ€æ—§çš„æ¶ˆæ¯ (FIFO)
                    dropped = tail_messages.pop(0)
                    dropped_tokens = self._count_tokens(str(dropped.get("content", "")))
                    total_tokens -= dropped_tokens

                    if self.valves.show_debug_log and __event_call__:
                        await self._log(
                            f"[Inlet] ğŸ—‘ï¸ ä»å†å²è®°å½•ä¸­ä¸¢å¼ƒæ¶ˆæ¯ä»¥é€‚åº”ä¸Šä¸‹æ–‡ã€‚è§’è‰²: {dropped.get('role')}, Tokens: {dropped_tokens}",
                            event_call=__event_call__,
                        )

                # é‡æ–°ç»„è£…
                candidate_messages = head_messages + [summary_msg] + tail_messages

                await self._log(
                    f"[Inlet] âœ‚ï¸ å†å²è®°å½•å·²ç¼©å‡ã€‚æ–°æ€»æ•°: {total_tokens} Tokens (å°¾éƒ¨å¤§å°: {len(tail_messages)})",
                    event_call=__event_call__,
                )

            final_messages = candidate_messages

            # è®¡ç®—è¯¦ç»† Token ç»Ÿè®¡ä»¥ç”¨äºæ—¥å¿—
            system_tokens = (
                self._count_tokens(system_prompt_msg.get("content", ""))
                if system_prompt_msg
                else 0
            )
            head_tokens = self._calculate_messages_tokens(head_messages)
            summary_tokens = self._count_tokens(summary_content)
            tail_tokens = self._calculate_messages_tokens(tail_messages)

            system_info = (
                f"System({system_tokens}t)" if system_prompt_msg else "System(0t)"
            )

            total_section_tokens = (
                system_tokens + head_tokens + summary_tokens + tail_tokens
            )

            await self._log(
                f"[Inlet] åº”ç”¨æ‘˜è¦: {system_info} + Head({len(head_messages)} æ¡, {head_tokens}t) + Summary({summary_tokens}t) + Tail({len(tail_messages)} æ¡, {tail_tokens}t) = Total({total_section_tokens}t)",
                type="success",
                event_call=__event_call__,
            )

            # å‡†å¤‡çŠ¶æ€æ¶ˆæ¯ (ä¸Šä¸‹æ–‡ä½¿ç”¨é‡æ ¼å¼)
            if max_context_tokens > 0:
                usage_ratio = total_section_tokens / max_context_tokens
                status_msg = f"ä¸Šä¸‹æ–‡ä½¿ç”¨é‡ (é¢„ä¼°): {total_section_tokens} / {max_context_tokens} Tokens ({usage_ratio*100:.1f}%)"
                if usage_ratio > 0.9:
                    status_msg += " | âš ï¸ é«˜è´Ÿè½½"
            else:
                status_msg = f"å·²åŠ è½½å†å²æ‘˜è¦ (éšè— {compressed_count} æ¡å†å²æ¶ˆæ¯)"

            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": status_msg,
                            "done": True,
                        },
                    }
                )

            # Emit debug log to frontend (Keep the structured log as well)
            await self._emit_debug_log(
                __event_call__,
                chat_id,
                len(messages),
                len(final_messages),
                len(summary_record.summary),
                self.valves.keep_first,
                self.valves.keep_last,
            )
        else:
            # æ²¡æœ‰æ‘˜è¦ï¼Œä½¿ç”¨åŸå§‹æ¶ˆæ¯
            # ä½†ä»ç„¶éœ€è¦æ£€æŸ¥é¢„ç®—ï¼
            final_messages = messages

            # åŒ…å«ç³»ç»Ÿæç¤ºè¯è¿›è¡Œè®¡ç®—
            calc_messages = final_messages
            if system_prompt_msg:
                is_in_messages = any(m.get("role") == "system" for m in final_messages)
                if not is_in_messages:
                    calc_messages = [system_prompt_msg] + final_messages

            # è·å–æœ€å¤§ä¸Šä¸‹æ–‡é™åˆ¶
            model = self._clean_model_id(body.get("model"))
            thresholds = self._get_model_thresholds(model)
            max_context_tokens = thresholds.get(
                "max_context_tokens", self.valves.max_context_tokens
            )

            total_tokens = await asyncio.to_thread(
                self._calculate_messages_tokens, calc_messages
            )

            if total_tokens > max_context_tokens:
                await self._log(
                    f"[Inlet] âš ï¸ åŸå§‹æ¶ˆæ¯ ({total_tokens} Tokens) è¶…è¿‡ä¸Šé™ ({max_context_tokens})ã€‚æ­£åœ¨ç¼©å‡å†å²è®°å½•...",
                    type="warning",
                    event_call=__event_call__,
                )

                # åŠ¨æ€ä»å¼€å¤´ç§»é™¤æ¶ˆæ¯
                # æˆ‘ä»¬å°†éµå®ˆ effective_keep_first ä»¥ä¿æŠ¤ç³»ç»Ÿæç¤ºè¯

                start_trim_index = effective_keep_first

                while (
                    total_tokens > max_context_tokens
                    and len(final_messages)
                    > start_trim_index + 1  # ä¿ç•™ keep_first ä¹‹åè‡³å°‘ 1 æ¡æ¶ˆæ¯
                ):
                    dropped = final_messages.pop(start_trim_index)
                    total_tokens -= self._count_tokens(str(dropped.get("content", "")))

                await self._log(
                    f"[Inlet] âœ‚ï¸ æ¶ˆæ¯å·²ç¼©å‡ã€‚æ–°æ€»æ•°: {total_tokens} Tokens",
                    event_call=__event_call__,
                )

            # å‘é€çŠ¶æ€é€šçŸ¥ (ä¸Šä¸‹æ–‡ä½¿ç”¨é‡æ ¼å¼)
            if __event_emitter__:
                status_msg = (
                    f"ä¸Šä¸‹æ–‡ä½¿ç”¨é‡ (é¢„ä¼°): {total_tokens} / {max_context_tokens} Tokens"
                )
                if max_context_tokens > 0:
                    usage_ratio = total_tokens / max_context_tokens
                    status_msg += f" ({usage_ratio*100:.1f}%)"
                    if usage_ratio > 0.9:
                        status_msg += " | âš ï¸ é«˜è´Ÿè½½"

                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": status_msg,
                            "done": True,
                        },
                    }
                )

        body["messages"] = final_messages

        await self._log(
            f"[Inlet] æœ€ç»ˆå‘é€: {len(body['messages'])} æ¡æ¶ˆæ¯\n{'='*60}\n",
            event_call=__event_call__,
        )

        return body

    async def outlet(
        self,
        body: dict,
        __user__: Optional[dict] = None,
        __metadata__: dict = None,
        __event_emitter__: Callable[[Any], Awaitable[None]] = None,
        __event_call__: Callable[[Any], Awaitable[None]] = None,
    ) -> dict:
        """
        åœ¨ LLM å“åº”å®Œæˆåæ‰§è¡Œ
        åœ¨åå°è®¡ç®— Token æ•°å¹¶è§¦å‘æ‘˜è¦ç”Ÿæˆï¼ˆä¸é˜»å¡å½“å‰å“åº”ï¼Œä¸å½±å“å†…å®¹è¾“å‡ºï¼‰
        """
        chat_ctx = self._get_chat_context(body, __metadata__)
        chat_id = chat_ctx["chat_id"]
        if not chat_id:
            await self._log(
                "[Outlet] âŒ metadata ä¸­ç¼ºå°‘ chat_idï¼Œè·³è¿‡å‹ç¼©",
                type="error",
                event_call=__event_call__,
            )
            return body
        model = body.get("model") or ""

        # ç›´æ¥è®¡ç®—ç›®æ ‡å‹ç¼©è¿›åº¦
        # å‡è®¾ outlet ä¸­çš„ body['messages'] åŒ…å«å®Œæ•´å†å²ï¼ˆåŒ…æ‹¬æ–°å“åº”ï¼‰
        messages = body.get("messages", [])
        target_compressed_count = max(0, len(messages) - self.valves.keep_last)

        if self.valves.debug_mode or self.valves.show_debug_log:
            await self._log(
                f"\n{'='*60}\n[Outlet] Chat ID: {chat_id}\n[Outlet] å“åº”å®Œæˆ\n[Outlet] è®¡ç®—ç›®æ ‡å‹ç¼©è¿›åº¦: {target_compressed_count} (æ¶ˆæ¯æ•°: {len(messages)})",
                event_call=__event_call__,
            )

        # åœ¨åå°å¼‚æ­¥å¤„ç† Token è®¡ç®—å’Œæ‘˜è¦ç”Ÿæˆï¼ˆä¸ç­‰å¾…å®Œæˆï¼Œä¸å½±å“è¾“å‡ºï¼‰
        asyncio.create_task(
            self._check_and_generate_summary_async(
                chat_id,
                model,
                body,
                __user__,
                target_compressed_count,
                __event_emitter__,
                __event_call__,
            )
        )

        await self._log(
            f"[Outlet] åå°å¤„ç†å·²å¯åŠ¨\n{'='*60}\n",
            event_call=__event_call__,
        )

        return body

    async def _check_and_generate_summary_async(
        self,
        chat_id: str,
        model: str,
        body: dict,
        user_data: Optional[dict],
        target_compressed_count: Optional[int],
        __event_emitter__: Callable[[Any], Awaitable[None]] = None,
        __event_call__: Callable[[Any], Awaitable[None]] = None,
    ):
        """
        åå°å¤„ç†ï¼šè®¡ç®— Token æ•°å¹¶ç”Ÿæˆæ‘˜è¦ï¼ˆä¸é˜»å¡å“åº”ï¼‰
        """
        try:
            messages = body.get("messages", [])

            # è·å–å½“å‰æ¨¡å‹çš„é˜ˆå€¼é…ç½®
            thresholds = self._get_model_thresholds(model)
            compression_threshold_tokens = thresholds.get(
                "compression_threshold_tokens", self.valves.compression_threshold_tokens
            )

            await self._log(
                f"\n[ğŸ” åå°è®¡ç®—] å¼€å§‹ Token è®¡æ•°...",
                event_call=__event_call__,
            )

            # åœ¨åå°çº¿ç¨‹ä¸­è®¡ç®— Token æ•°
            current_tokens = await asyncio.to_thread(
                self._calculate_messages_tokens, messages
            )

            await self._log(
                f"[ğŸ” åå°è®¡ç®—] Token æ•°: {current_tokens}",
                event_call=__event_call__,
            )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©
            if current_tokens >= compression_threshold_tokens:
                await self._log(
                    f"[ğŸ” åå°è®¡ç®—] âš¡ è§¦å‘å‹ç¼©é˜ˆå€¼ (Token: {current_tokens} >= {compression_threshold_tokens})",
                    type="warning",
                    event_call=__event_call__,
                )

                # ç»§ç»­ç”Ÿæˆæ‘˜è¦
                await self._generate_summary_async(
                    messages,
                    chat_id,
                    body,
                    user_data,
                    target_compressed_count,
                    __event_emitter__,
                    __event_call__,
                )
            else:
                await self._log(
                    f"[ğŸ” åå°è®¡ç®—] æœªè§¦å‘å‹ç¼©é˜ˆå€¼ (Token: {current_tokens} < {compression_threshold_tokens})",
                    event_call=__event_call__,
                )

        except Exception as e:
            await self._log(
                f"[ğŸ” åå°è®¡ç®—] âŒ é”™è¯¯: {str(e)}",
                type="error",
                event_call=__event_call__,
            )

    def _clean_model_id(self, model_id: Optional[str]) -> Optional[str]:
        """Cleans the model ID by removing whitespace and quotes."""
        if not model_id:
            return None
        cleaned = model_id.strip().strip('"').strip("'")
        return cleaned if cleaned else None

    async def _generate_summary_async(
        self,
        messages: list,
        chat_id: str,
        body: dict,
        user_data: Optional[dict],
        target_compressed_count: Optional[int],
        __event_emitter__: Callable[[Any], Awaitable[None]] = None,
        __event_call__: Callable[[Any], Awaitable[None]] = None,
    ):
        """
        å¼‚æ­¥ç”Ÿæˆæ‘˜è¦ï¼ˆåå°æ‰§è¡Œï¼Œä¸é˜»å¡å“åº”ï¼‰
        é€»è¾‘ï¼š
        1. æå–ä¸­é—´æ¶ˆæ¯ï¼ˆå»é™¤ keep_first å’Œ keep_lastï¼‰ã€‚
        2. æ£€æŸ¥ Token ä¸Šé™ï¼Œå¦‚æœè¶…è¿‡ max_context_tokensï¼Œä»ä¸­é—´æ¶ˆæ¯å¤´éƒ¨ç§»é™¤ã€‚
        3. å¯¹å‰©ä½™çš„ä¸­é—´æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦ã€‚
        """
        try:
            await self._log(f"\n[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] å¼€å§‹...", event_call=__event_call__)

            # 1. è·å–ç›®æ ‡å‹ç¼©è¿›åº¦
            # å¦‚æœæœªä¼ é€’ target_compressed_countï¼ˆæ–°é€»è¾‘ä¸‹ä¸åº”å‘ç”Ÿï¼‰ï¼Œåˆ™è¿›è¡Œä¼°ç®—
            if target_compressed_count is None:
                target_compressed_count = max(0, len(messages) - self.valves.keep_last)
                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âš ï¸ target_compressed_count ä¸º Noneï¼Œè¿›è¡Œä¼°ç®—: {target_compressed_count}",
                    type="warning",
                    event_call=__event_call__,
                )

            # 2. ç¡®å®šå¾…å‹ç¼©çš„æ¶ˆæ¯èŒƒå›´ (Middle)
            start_index = self.valves.keep_first
            end_index = len(messages) - self.valves.keep_last
            if self.valves.keep_last == 0:
                end_index = len(messages)

            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            if start_index >= end_index:
                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] ä¸­é—´æ¶ˆæ¯ä¸ºç©º (Start: {start_index}, End: {end_index})ï¼Œè·³è¿‡",
                    event_call=__event_call__,
                )
                return

            middle_messages = messages[start_index:end_index]
            tail_preview_msgs = messages[end_index:]

            if self.valves.show_debug_log and __event_call__:
                middle_preview = [
                    f"{i + start_index}: [{m.get('role')}] {m.get('content', '')[:20]}..."
                    for i, m in enumerate(middle_messages[:3])
                ]
                tail_preview = [
                    f"{i + end_index}: [{m.get('role')}] {m.get('content', '')[:20]}..."
                    for i, m in enumerate(tail_preview_msgs)
                ]
                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] ğŸ“Š è¾¹ç•Œæ£€æŸ¥:\n"
                    f"  - ä¸­é—´ (å‹ç¼©): {len(middle_messages)} æ¡ (ç´¢å¼• {start_index}-{end_index-1}) -> é¢„è§ˆ: {middle_preview}\n"
                    f"  - å°¾éƒ¨ (ä¿ç•™): {len(tail_preview_msgs)} æ¡ (ç´¢å¼• {end_index}-End) -> é¢„è§ˆ: {tail_preview}",
                    event_call=__event_call__,
                )

            # 3. æ£€æŸ¥ Token ä¸Šé™å¹¶æˆªæ–­ (Max Context Truncation)
            # [ä¼˜åŒ–] ä½¿ç”¨æ‘˜è¦æ¨¡å‹(å¦‚æœæœ‰)çš„é˜ˆå€¼æ¥å†³å®šèƒ½å¤„ç†å¤šå°‘ä¸­é—´æ¶ˆæ¯
            # è¿™æ ·å¯ä»¥ç”¨é•¿çª—å£æ¨¡å‹(å¦‚ gemini-flash)æ¥å‹ç¼©è¶…è¿‡å½“å‰æ¨¡å‹çª—å£çš„å†å²è®°å½•
            summary_model_id = self._clean_model_id(
                self.valves.summary_model
            ) or self._clean_model_id(body.get("model"))

            if not summary_model_id:
                await self._log(
                    "[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âš ï¸ æ‘˜è¦æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡å‹ç¼©",
                    type="warning",
                    event_call=__event_call__,
                )
                return

            thresholds = self._get_model_thresholds(summary_model_id)
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯æ‘˜è¦æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é™åˆ¶
            max_context_tokens = thresholds.get(
                "max_context_tokens", self.valves.max_context_tokens
            )

            await self._log(
                f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] ä½¿ç”¨æ¨¡å‹ {summary_model_id} çš„ä¸Šé™: {max_context_tokens} Tokens",
                event_call=__event_call__,
            )

            # è®¡ç®—ä¸­é—´æ¶ˆæ¯çš„ Token (åŠ ä¸Šæç¤ºè¯çš„ç¼“å†²)
            # æˆ‘ä»¬åªæŠŠ middle_messages å‘é€ç»™æ‘˜è¦æ¨¡å‹ï¼Œæ‰€ä»¥ä¸åº”è¯¥æŠŠå®Œæ•´å†å²è®¡å…¥é™åˆ¶
            middle_tokens = await asyncio.to_thread(
                self._calculate_messages_tokens, middle_messages
            )
            # å¢åŠ æç¤ºè¯å’Œè¾“å‡ºçš„ç¼“å†² (çº¦ 2000 Tokens)
            estimated_input_tokens = middle_tokens + 2000

            if estimated_input_tokens > max_context_tokens:
                excess_tokens = estimated_input_tokens - max_context_tokens
                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âš ï¸ ä¸­é—´æ¶ˆæ¯ ({middle_tokens} Tokens) + ç¼“å†²è¶…è¿‡æ‘˜è¦æ¨¡å‹ä¸Šé™ ({max_context_tokens})ï¼Œéœ€è¦ç§»é™¤çº¦ {excess_tokens} Token",
                    type="warning",
                    event_call=__event_call__,
                )

                # ä» middle_messages å¤´éƒ¨å¼€å§‹ç§»é™¤
                removed_tokens = 0
                removed_count = 0

                while removed_tokens < excess_tokens and middle_messages:
                    msg_to_remove = middle_messages.pop(0)
                    msg_tokens = self._count_tokens(
                        str(msg_to_remove.get("content", ""))
                    )
                    removed_tokens += msg_tokens
                    removed_count += 1

                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] å·²ç§»é™¤ {removed_count} æ¡æ¶ˆæ¯ï¼Œå…± {removed_tokens} Token",
                    event_call=__event_call__,
                )

            if not middle_messages:
                await self._log(
                    f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] æˆªæ–­åä¸­é—´æ¶ˆæ¯ä¸ºç©ºï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ",
                    event_call=__event_call__,
                )
                return

            # 4. æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation_text = self._format_messages_for_summary(middle_messages)

            # 5. è°ƒç”¨ LLM ç”Ÿæˆæ–°æ‘˜è¦
            # æ³¨æ„ï¼šè¿™é‡Œä¸å†ä¼ å…¥ previous_summaryï¼Œå› ä¸ºæ—§æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰å·²ç»åŒ…å«åœ¨ middle_messages é‡Œäº†

            # å‘é€å¼€å§‹ç”Ÿæˆæ‘˜è¦çš„çŠ¶æ€é€šçŸ¥
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": "æ­£åœ¨åå°ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦...",
                            "done": False,
                        },
                    }
                )

            new_summary = await self._call_summary_llm(
                None,
                conversation_text,
                {**body, "model": summary_model_id},
                user_data,
                __event_call__,
            )

            if not new_summary:
                await self._log(
                    "[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âš ï¸ æ‘˜è¦ç”Ÿæˆè¿”å›ç©ºç»“æœï¼Œè·³è¿‡ä¿å­˜",
                    type="warning",
                    event_call=__event_call__,
                )
                return

            # 6. ä¿å­˜æ–°æ‘˜è¦
            await self._log(
                "[ä¼˜åŒ–] åœ¨åå°çº¿ç¨‹ä¸­ä¿å­˜æ‘˜è¦ä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ã€‚",
                event_call=__event_call__,
            )

            await asyncio.to_thread(
                self._save_summary, chat_id, new_summary, target_compressed_count
            )

            # å‘é€å®ŒæˆçŠ¶æ€é€šçŸ¥
            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": f"ä¸Šä¸‹æ–‡æ‘˜è¦å·²æ›´æ–° (å‹ç¼©äº† {len(middle_messages)} æ¡æ¶ˆæ¯)",
                            "done": True,
                        },
                    }
                )

            await self._log(
                f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âœ… å®Œæˆï¼æ–°æ‘˜è¦é•¿åº¦: {len(new_summary)} å­—ç¬¦",
                type="success",
                event_call=__event_call__,
            )
            await self._log(
                f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] è¿›åº¦æ›´æ–°: å·²å‹ç¼©è‡³åŸå§‹æ¶ˆæ¯ {target_compressed_count}",
                event_call=__event_call__,
            )

            # --- Token ä½¿ç”¨æƒ…å†µçŠ¶æ€é€šçŸ¥ ---
            if self.valves.show_token_usage_status and __event_emitter__:
                try:
                    # 1. è·å–ç³»ç»Ÿæç¤ºè¯ (DB å›é€€)
                    system_prompt_msg = None
                    model_id = body.get("model")
                    if model_id:
                        try:
                            model_obj = Models.get_model_by_id(model_id)
                            if model_obj and model_obj.params:
                                params = model_obj.params
                                if isinstance(params, str):
                                    params = json.loads(params)
                                if isinstance(params, dict):
                                    sys_content = params.get("system")
                                else:
                                    sys_content = getattr(params, "system", None)

                                if sys_content:
                                    system_prompt_msg = {
                                        "role": "system",
                                        "content": sys_content,
                                    }
                        except Exception:
                            pass  # å¿½ç•¥ DB é”™è¯¯ï¼Œå°½åŠ›è€Œä¸º

                    # 2. è®¡ç®— Effective Keep First
                    last_system_index = -1
                    for i, msg in enumerate(messages):
                        if msg.get("role") == "system":
                            last_system_index = i
                    effective_keep_first = max(
                        self.valves.keep_first, last_system_index + 1
                    )

                    # 3. æ„å»ºä¸‹ä¸€ä¸ªä¸Šä¸‹æ–‡ (Next Context)
                    # Head
                    head_msgs = (
                        messages[:effective_keep_first]
                        if effective_keep_first > 0
                        else []
                    )

                    # Summary
                    summary_content = (
                        f"ã€ç³»ç»Ÿæç¤ºï¼šä»¥ä¸‹æ˜¯å†å²å¯¹è¯çš„æ‘˜è¦ï¼Œä»…ä¾›å‚è€ƒä¸Šä¸‹æ–‡ï¼Œè¯·å‹¿å¯¹æ‘˜è¦å†…å®¹è¿›è¡Œå›å¤ï¼Œç›´æ¥å›ç­”åç»­çš„æœ€æ–°é—®é¢˜ã€‘\n\n"
                        f"{new_summary}\n\n"
                        f"---\n"
                        f"ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯ï¼š"
                    )
                    summary_msg = {"role": "assistant", "content": summary_content}

                    # Tail (ä½¿ç”¨ target_compressed_countï¼Œè¿™æ˜¯æˆ‘ä»¬åˆšåˆšå‹ç¼©åˆ°çš„ä½ç½®)
                    # æ³¨æ„ï¼štarget_compressed_count æ˜¯è¦è¢«æ‘˜è¦è¦†ç›–çš„æ¶ˆæ¯æ•°ï¼ˆä¸åŒ…æ‹¬ keep_lastï¼‰
                    # æ‰€ä»¥ tail ä» max(target_compressed_count, effective_keep_first) å¼€å§‹
                    start_index = max(target_compressed_count, effective_keep_first)
                    tail_msgs = messages[start_index:]

                    # ç»„è£…
                    next_context = head_msgs + [summary_msg] + tail_msgs

                    # å¦‚æœéœ€è¦ï¼Œæ³¨å…¥ç³»ç»Ÿæç¤ºè¯
                    if system_prompt_msg:
                        is_in_head = any(m.get("role") == "system" for m in head_msgs)
                        if not is_in_head:
                            next_context = [system_prompt_msg] + next_context

                    # 4. è®¡ç®— Token
                    token_count = self._calculate_messages_tokens(next_context)

                    # 5. è·å–é˜ˆå€¼å¹¶è®¡ç®—æ¯”ä¾‹
                    model = self._clean_model_id(body.get("model"))
                    thresholds = self._get_model_thresholds(model)
                    max_context_tokens = thresholds.get(
                        "max_context_tokens", self.valves.max_context_tokens
                    )

                    # 6. å‘é€çŠ¶æ€
                    status_msg = (
                        f"ä¸Šä¸‹æ–‡æ‘˜è¦å·²æ›´æ–°: {token_count} / {max_context_tokens} Tokens"
                    )
                    if max_context_tokens > 0:
                        ratio = (token_count / max_context_tokens) * 100
                        status_msg += f" ({ratio:.1f}%)"
                        if ratio > 90.0:
                            status_msg += " | âš ï¸ é«˜è´Ÿè½½"

                    await __event_emitter__(
                        {
                            "type": "status",
                            "data": {
                                "description": status_msg,
                                "done": True,
                            },
                        }
                    )
                except Exception as e:
                    await self._log(
                        f"[Status] è®¡ç®— Token é”™è¯¯: {e}",
                        type="error",
                        event_call=__event_call__,
                    )

        except Exception as e:
            await self._log(
                f"[ğŸ¤– å¼‚æ­¥æ‘˜è¦ä»»åŠ¡] âŒ é”™è¯¯: {str(e)}",
                type="error",
                event_call=__event_call__,
            )

            if __event_emitter__:
                await __event_emitter__(
                    {
                        "type": "status",
                        "data": {
                            "description": f"æ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)[:100]}...",
                            "done": True,
                        },
                    }
                )

            import traceback

            traceback.print_exc()

    def _format_messages_for_summary(self, messages: list) -> str:
        """Formats messages for summarization."""
        formatted = []
        for i, msg in enumerate(messages, 1):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")

            # Handle multimodal content
            if isinstance(content, list):
                text_parts = []
                for part in content:
                    if isinstance(part, dict) and part.get("type") == "text":
                        text_parts.append(part.get("text", ""))
                content = " ".join(text_parts)

            # Handle role name
            role_name = {"user": "User", "assistant": "Assistant"}.get(role, role)

            # Limit length of each message to avoid excessive length
            if len(content) > 500:
                content = content[:500] + "..."

            formatted.append(f"[{i}] {role_name}: {content}")

        return "\n\n".join(formatted)

    async def _call_summary_llm(
        self,
        previous_summary: Optional[str],
        new_conversation_text: str,
        body: dict,
        user_data: dict,
        __event_call__: Callable[[Any], Awaitable[None]] = None,
    ) -> str:
        """
        è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦ï¼Œä½¿ç”¨ Open Web UI çš„å†…ç½®æ–¹æ³•ã€‚
        """
        await self._log(
            f"[ğŸ¤– LLM è°ƒç”¨] ä½¿ç”¨ Open Web UI å†…ç½®æ–¹æ³•",
            event_call=__event_call__,
        )

        # æ„å»ºæ‘˜è¦æç¤ºè¯ (ä¼˜åŒ–ç‰ˆ)
        summary_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯ä¸Šä¸‹æ–‡å‹ç¼©ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹ä»¥ä¸‹å¯¹è¯å†…å®¹è¿›è¡Œé«˜ä¿çœŸæ‘˜è¦ã€‚
è¿™æ®µå¯¹è¯å¯èƒ½åŒ…å«ä¹‹å‰çš„æ‘˜è¦ï¼ˆä½œä¸ºç³»ç»Ÿæ¶ˆæ¯æˆ–æ–‡æœ¬ï¼‰ä»¥åŠåç»­çš„å¯¹è¯å†…å®¹ã€‚

### æ ¸å¿ƒç›®æ ‡
1.  **å…¨é¢æ€»ç»“**ï¼šå°†å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯ã€ç”¨æˆ·æ„å›¾ã€åŠ©æ‰‹å›å¤è¿›è¡Œç²¾ç‚¼æ€»ç»“ã€‚
2.  **å»å™ªæçº¯**ï¼šç§»é™¤å¯’æš„ã€é‡å¤ã€ç¡®è®¤æ€§å›å¤ç­‰æ— ç”¨ä¿¡æ¯ã€‚
3.  **å…³é”®ä¿ç•™**ï¼š
    *   **ä»£ç ç‰‡æ®µã€å‘½ä»¤ã€æŠ€æœ¯å‚æ•°å¿…é¡»é€å­—ä¿ç•™ï¼Œä¸¥ç¦ä¿®æ”¹æˆ–æ¦‚æ‹¬ã€‚**
    *   ç”¨æˆ·æ„å›¾ã€æ ¸å¿ƒéœ€æ±‚ã€å†³ç­–ç»“è®ºã€å¾…åŠäº‹é¡¹å¿…é¡»æ¸…æ™°ä¿ç•™ã€‚
4.  **è¿è´¯æ€§**ï¼šç”Ÿæˆçš„æ‘˜è¦åº”ä½œä¸ºä¸€ä¸ªæ•´ä½“ï¼Œèƒ½å¤Ÿæ›¿ä»£åŸå§‹å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
5.  **è¯¦å°½è®°å½•**ï¼šç”±äºå…è®¸çš„ç¯‡å¹…è¾ƒé•¿ï¼Œè¯·å°½å¯èƒ½ä¿ç•™å¯¹è¯ä¸­çš„ç»†èŠ‚ã€è®ºè¯è¿‡ç¨‹å’Œå¤šè½®äº¤äº’çš„ç»†å¾®å·®åˆ«ï¼Œè€Œä¸ä»…ä»…æ˜¯å®è§‚æ¦‚æ‹¬ã€‚

### è¾“å‡ºè¦æ±‚
*   **æ ¼å¼**ï¼šç»“æ„åŒ–æ–‡æœ¬ï¼Œé€»è¾‘æ¸…æ™°ã€‚
*   **è¯­è¨€**ï¼šä¸å¯¹è¯è¯­è¨€ä¿æŒä¸€è‡´ï¼ˆé€šå¸¸ä¸ºä¸­æ–‡ï¼‰ã€‚
*   **é•¿åº¦**ï¼šä¸¥æ ¼æ§åˆ¶åœ¨ {self.valves.max_summary_tokens} Token ä»¥å†…ã€‚
*   **ä¸¥ç¦**ï¼šä¸è¦è¾“å‡º"æ ¹æ®å¯¹è¯..."ã€"æ‘˜è¦å¦‚ä¸‹..."ç­‰åºŸè¯ã€‚ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ã€‚

### æ‘˜è¦ç»“æ„å»ºè®®
*   **å½“å‰ç›®æ ‡/ä¸»é¢˜**ï¼šä¸€å¥è¯æ¦‚æ‹¬å½“å‰æ­£åœ¨è§£å†³çš„é—®é¢˜ã€‚
*   **å…³é”®ä¿¡æ¯ä¸ä¸Šä¸‹æ–‡**ï¼š
    *   å·²ç¡®è®¤çš„äº‹å®/å‚æ•°ã€‚
    *   **ä»£ç /æŠ€æœ¯ç»†èŠ‚** (ä½¿ç”¨ä»£ç å—åŒ…è£¹)ã€‚
*   **è¿›å±•ä¸ç»“è®º**ï¼šå·²å®Œæˆçš„æ­¥éª¤å’Œè¾¾æˆçš„å…±è¯†ã€‚
*   **å¾…åŠ/ä¸‹ä¸€æ­¥**ï¼šæ˜ç¡®çš„åç»­è¡ŒåŠ¨ã€‚

---
{new_conversation_text}
---

è¯·æ ¹æ®ä¸Šè¿°å†…å®¹ï¼Œç”Ÿæˆæ‘˜è¦ï¼š
"""
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        model = self._clean_model_id(self.valves.summary_model) or self._clean_model_id(
            body.get("model")
        )

        if not model:
            await self._log(
                "[ğŸ¤– LLM è°ƒç”¨] âš ï¸ æ‘˜è¦æ¨¡å‹ä¸å­˜åœ¨ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ",
                type="warning",
                event_call=__event_call__,
            )
            return ""

        await self._log(f"[ğŸ¤– LLM è°ƒç”¨] æ¨¡å‹: {model}", event_call=__event_call__)

        # æ„å»º payload
        payload = {
            "model": model,
            "messages": [{"role": "user", "content": summary_prompt}],
            "stream": False,
            "max_tokens": self.valves.max_summary_tokens,
            "temperature": self.valves.summary_temperature,
        }

        try:
            # è·å–ç”¨æˆ·å¯¹è±¡
            user_id = user_data.get("id") if user_data else None
            if not user_id:
                raise ValueError("æ— æ³•è·å–ç”¨æˆ· ID")

            # [ä¼˜åŒ–] åœ¨åå°çº¿ç¨‹ä¸­è·å–ç”¨æˆ·å¯¹è±¡ä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            await self._log(
                "[ä¼˜åŒ–] åœ¨åå°çº¿ç¨‹ä¸­è·å–ç”¨æˆ·å¯¹è±¡ä»¥é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ã€‚",
                event_call=__event_call__,
            )
            user = await asyncio.to_thread(Users.get_user_by_id, user_id)

            if not user:
                raise ValueError(f"æ— æ³•æ‰¾åˆ°ç”¨æˆ·: {user_id}")

            await self._log(
                f"[ğŸ¤– LLM è°ƒç”¨] ç”¨æˆ·: {user.email}\n[ğŸ¤– LLM è°ƒç”¨] å‘é€è¯·æ±‚...",
                event_call=__event_call__,
            )

            # åˆ›å»º Request å¯¹è±¡
            request = Request(scope={"type": "http", "app": webui_app})

            # è°ƒç”¨ generate_chat_completion
            response = await generate_chat_completion(request, payload, user)

            if not response or "choices" not in response or not response["choices"]:
                raise ValueError("LLM å“åº”æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º")

            summary = response["choices"][0]["message"]["content"].strip()

            await self._log(
                f"[ğŸ¤– LLM è°ƒç”¨] âœ… æˆåŠŸæ¥æ”¶æ‘˜è¦",
                type="success",
                event_call=__event_call__,
            )

            return summary

        except Exception as e:
            error_msg = str(e)
            # Handle specific error messages
            if "Model not found" in error_msg:
                error_message = f"æ‘˜è¦æ¨¡å‹ '{model}' ä¸å­˜åœ¨ã€‚"
            else:
                error_message = f"æ‘˜è¦ LLM é”™è¯¯ ({model}): {error_msg}"
            if not self.valves.summary_model:
                error_message += (
                    "\n[æç¤º] æ‚¨æœªæŒ‡å®š summary_modelï¼Œå› æ­¤è¿‡æ»¤å™¨å°è¯•ä½¿ç”¨å½“å‰å¯¹è¯çš„æ¨¡å‹ã€‚"
                    "å¦‚æœè¿™æ˜¯æµæ°´çº¿ (Pipe) æ¨¡å‹æˆ–ä¸å…¼å®¹çš„æ¨¡å‹ï¼Œè¯·åœ¨é…ç½®ä¸­æŒ‡å®šå…¼å®¹çš„æ‘˜è¦æ¨¡å‹ (ä¾‹å¦‚ 'gemini-2.5-flash')ã€‚"
                )

            await self._log(
                f"[ğŸ¤– LLM è°ƒç”¨] âŒ {error_message}",
                type="error",
                event_call=__event_call__,
            )

            raise Exception(error_message)
